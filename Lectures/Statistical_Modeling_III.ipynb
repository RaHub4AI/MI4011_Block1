{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e728ea0b",
   "metadata": {},
   "source": [
    "# Statistical Modeling III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7f386",
   "metadata": {},
   "source": [
    "In the previous session, we focused on **linear regression**, starting with a **single predictor** and then extending the idea to **multiple linear regression (MLR)**. We learned how to **fit regression models using Ordinary Least Squares (OLS)**, how to **assess overall model significance**, and how to determine **which predictors meaningfully contribute** to explaining variation in the response variable.\n",
    "\n",
    "Today, we build directly on that foundation by adding a crucial new component: **uncertainty**. While point estimates give us a single predicted value, they do not tell us how reliable that prediction is.\n",
    "\n",
    "We will learn how to **quantify uncertainty in model predictions** using **confidence intervals** and **prediction intervals**. These tools allow us to answer important practical questions such as:\n",
    "\n",
    "- *How confident are we about the **mean response** at a given value of the predictor?*\n",
    "- *How much variability should we expect in **individual future observations**?*\n",
    "\n",
    "Understanding the difference between these intervals is essential for interpreting regression results correctly and for making statistically sound conclusions from our models.\n",
    "\n",
    "Before introducing these concepts, we will first fit the same **simple linear regression model** as in the previous session, using the **Palmer Penguins dataset**. In this model, **flipper length** is the independent variable (predictor), and **body mass** is the dependent variable (response).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51edfef2",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 1: Fitting a simple linear regression model</font>\n",
    "\n",
    "Let’s load the dataset and fit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Palmer Penguins dataset and remove observations with missing values in flipper length or body mass to ensure a clean regression analysis\n",
    "penguins = sns.load_dataset('penguins').dropna(subset=['flipper_length_mm', 'body_mass_g']).reset_index(drop=True)\n",
    "\n",
    "# Fit a simple linear regression model using Ordinary Least Squares (OLS), with body mass as the response variable and flipper length as the predicto\n",
    "model1 = smf.ols('body_mass_g ~ flipper_length_mm', data=penguins).fit()\n",
    "\n",
    "# Display a detailed summary of the fitted regression model\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348aa308",
   "metadata": {},
   "source": [
    "#### Confidence Intervals (CIs)\n",
    "\n",
    "So far, we have already interpreted several parts of the regression output. But there is one more *very important* piece of information in the model summary: the columns with the brackets:\n",
    "\n",
    "`[0.025      0.975]`\n",
    "\n",
    "The numbers in these columns give us the **confidence intervals (CIs)** for the regression coefficients reported in the summary.\n",
    "\n",
    "> **Why is this a 95% confidence interval?**\n",
    ">\n",
    "> The two values $0.025$ and $0.975$ correspond to the **2.5th percentile** and the **97.5th percentile** of the sampling distribution.  \n",
    "> This means we keep the middle **95%** of the distribution and leave out **2.5% in each tail**: $0.975 - 0.025 = 0.95$\n",
    ">\n",
    "> So the interval shown in the output is a **95% confidence interval**.\n",
    "\n",
    "In regression, a confidence interval provides a **range of plausible values** for the *true population parameter* (for example, the true $\\beta_1$), based on our sample data.\n",
    "\n",
    "A **95% CI for $\\hat{\\beta}_1$** means: If we repeatedly collected new samples, fit the same regression model each time, and computed a 95% CI each time, then about **95% of those intervals** would contain the *true* $\\beta_1$.\n",
    "\n",
    "> In other words: The 95% CI reflects the **uncertainty** in our estimate. It tells us how precisely we have estimated the model’s coefficients.\n",
    "\n",
    "\n",
    "> **Link to statistical significance**\n",
    ">\n",
    "> If the confidence interval for a coefficient **does not include $0$**, it suggests that this predictor likely has a **statistically significant effect** on the response variable (at the $5\\%$ level).\\\n",
    "This matches the conclusion from the ***t*-test** and its ***p*-value** in the regression summary.\n",
    "\n",
    "Next, we will extract and interpret these confidence intervals directly in Python using our fitted model `model1`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0edb0",
   "metadata": {},
   "source": [
    "If we want the confidence intervals for our fitted model, we can use the model’s `.conf_int(alpha=...)` method, where  $\\alpha = 1 - \\text{confidence level}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.conf_int(alpha=0.05) # 95% CIs (same as shown in .summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a030e6",
   "metadata": {},
   "source": [
    "We are not restricted to a 95% confidence level. We can choose different confidence levels depending on the context and how much uncertainty we are willing to tolerate.\n",
    "\n",
    "For example, a 90% confidence interval corresponds to $\\alpha = 0.10$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d62d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.conf_int(alpha=0.10)  # 90% CIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674685ab",
   "metadata": {},
   "source": [
    "Lower confidence levels produce **narrower intervals**, while higher confidence levels produce **wider intervals**, reflecting a trade-off between precision and certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86cc6cb",
   "metadata": {},
   "source": [
    "##### Visualizing the confidence bands\n",
    "\n",
    "**Confidence bands** show how the **uncertainty of the estimated mean response** $\\hat{y}(x)$ changes across different values of $x$.  \n",
    "In other words, they illustrate how confident we are about the **average predicted value** of $y$ at each point along the regression line.\n",
    "\n",
    "We can compute and plot these confidence bands in Python using the model’s `.get_prediction()` and `.summary_frame()` methods.\n",
    "\n",
    "> The method `.get_prediction()` in `statsmodels` allows us to compute **predicted values** along with the associated **uncertainty estimates** for a regression model.  \n",
    "> By default, if no new data are provided, it returns predictions for all observations used to fit the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72877f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ols = model1.get_prediction()\n",
    "sf = pred_ols.summary_frame(alpha=0.05)\n",
    "sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f967509",
   "metadata": {},
   "source": [
    "> - The method `.get_prediction()` creates a prediction-results object that contains not only the fitted values $\\hat{y}_i$, but also all the necessary information to compute **standard errors** and **confidence or prediction intervals**.  \n",
    ">   The optional argument `exog=` can be used to specify new $x$ values.  \n",
    ">   Since we did not specify `exog`, it returns predictions for all data points used when fitting the model.\n",
    ">\n",
    "> - The `.summary_frame()` method then converts these prediction results into a **DataFrame** containing all the relevant quantities for each observation, including the fitted value, its standard error, and the bounds of both **confidence** and **prediction intervals**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "# Sort by flipper_length_mm so lines/bands render correctly\n",
    "order = np.argsort(penguins['flipper_length_mm'].values)\n",
    "x_sorted = penguins['flipper_length_mm'].values[order]\n",
    "y_sorted = penguins['body_mass_g'].values[order]\n",
    "mean_sorted = sf['mean'].values[order]\n",
    "lo_sorted = sf['mean_ci_lower'].values[order]\n",
    "hi_sorted = sf['mean_ci_upper'].values[order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], s=15, alpha=0.6, label='observed data', color='darkgray')\n",
    "ax.plot(x_sorted, mean_sorted, lw=2, color='#004aad', label='fitted line (mean)')\n",
    "ax.fill_between(x_sorted, lo_sorted, hi_sorted, alpha=0.2, color='#004aad', label='95% CI (mean)')\n",
    "ax.set_xlabel('Flipper length (mm)')\n",
    "ax.set_ylabel('Body mass (g)')\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e675f4",
   "metadata": {},
   "source": [
    "> As an alternative, we can create a very similar visualization using `seaborn` and its `regplot` function.  \n",
    "> This provides a convenient way to plot the **regression line together with its 95% confidence interval** using a single command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e51210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the scatterplot with regression line using seaborn\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "sns.regplot(data=penguins, x='flipper_length_mm', y='body_mass_g', color='darkgray',\n",
    "    ci=95,  # plot 95% confidence interval for the mean response\n",
    "    scatter_kws={'s': 15, 'alpha': 0.6},\n",
    "    line_kws={'color': '#004aad', 'lw': 2}\n",
    ")\n",
    "\n",
    "plt.xlabel('Flipper length (mm)')\n",
    "plt.ylabel('Body mass (g)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45dd9c",
   "metadata": {},
   "source": [
    "As you may notice, the **confidence bands are curved**. They are **narrowest near the center** of the data and **widen symmetrically toward both ends**. This reflects increasing uncertainty as we move away from the mean of $x$ (in this case, the mean value of flipper length).\n",
    "\n",
    "This curvature does **not** imply that the model allows for a curved relationship. The regression model itself is still **strictly linear**.\n",
    "\n",
    "The curved shape of the confidence bands represents the **envelope of all plausible straight lines** that could result from random sampling variability in the estimated intercept and slope.\n",
    "\n",
    "> To see *why* this happens, we can use a simple simulation approach. We will repeatedly:\n",
    "> - randomly sample 50 penguins from the dataset,\n",
    "> - fit the same simple linear regression model to each sample, and\n",
    "> - repeat this process many times (for example, 1000 times).\n",
    ">\n",
    "> Overlaying these fitted lines will show how uncertainty in the estimated coefficients leads to greater spread at the edges of the data range, producing the characteristic curved shape of the confidence bands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd71ab",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 2: Simulation of sampling variability in linear regression</font>\n",
    "\n",
    "To understand how this simulation works, we will start with a **single simulation step**.  \n",
    "We randomly select **50 penguins** from the dataset, fit a **simple linear regression model**, and visualize the fitted line together with the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e0904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly sample 50 penguins (with replacement)\n",
    "sample = penguins.sample(n=50, replace=True)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232883c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the simple linear regression model\n",
    "model_sim = smf.ols('body_mass_g ~ flipper_length_mm', data=sample).fit()\n",
    "\n",
    "# Create a grid of x values for plotting the fitted line\n",
    "x_grid = np.linspace(penguins['flipper_length_mm'].min(), penguins['flipper_length_mm'].max(), 100)\n",
    "\n",
    "# Predict body mass over the x grid\n",
    "y_pred = model_sim.predict({'flipper_length_mm': x_grid})\n",
    "\n",
    "# Plot original data (light gray)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], alpha=0.3, color='lightgray', label='Original data')\n",
    "\n",
    "\n",
    "# Plot sampled data (highlighted)\n",
    "plt.scatter(sample['flipper_length_mm'],sample['body_mass_g'], alpha=0.8, color='#fc7202', label='Sample (n = 50)')\n",
    "\n",
    "# Plot fitted regression line\n",
    "plt.plot(x_grid, y_pred, color='#fc7202', label='Fitted regression line')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Flipper length (mm)')\n",
    "plt.ylabel('Body mass (g)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1172b47",
   "metadata": {},
   "source": [
    "Now let’s take the next step.\n",
    "\n",
    "We will **draw a new random sample**, fit the same **simple linear regression model**, and **plot it on the same graph as the previous one**.  \n",
    "> Each new sample and its fitted line will be shown in **different colors**, while the original data remain unchanged in the background.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c9ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2)\n",
    "\n",
    "# New random sample of 50 penguins\n",
    "sample2 = penguins.sample(n=50, replace=True)\n",
    "\n",
    "# Fit the simple linear regression model\n",
    "model_sim2 = smf.ols('body_mass_g ~ flipper_length_mm', data=sample2).fit()\n",
    "\n",
    "# Create a grid of x values for plotting the fitted line\n",
    "x_grid = np.linspace(penguins['flipper_length_mm'].min(), penguins['flipper_length_mm'].max(), 100)\n",
    "\n",
    "# Predict body mass over the x grid\n",
    "y_pred2 = model_sim2.predict({'flipper_length_mm': x_grid})\n",
    "\n",
    "# Plot original data (light gray)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], alpha=0.3, color='lightgray', label='Original data')\n",
    "\n",
    "# Plot first sample and fitted line\n",
    "plt.scatter(sample['flipper_length_mm'],sample['body_mass_g'], alpha=0.8, color='#fc7202', label='Sample 1 (n = 50)')\n",
    "plt.plot(x_grid, y_pred, color='#fc7202', label='Fitted regression line 1')\n",
    "\n",
    "# Plot second sample and fitted line\n",
    "plt.scatter(sample2['flipper_length_mm'],sample2['body_mass_g'], alpha=0.8, color='#004aad', label='Sample 2 (n = 50)')\n",
    "plt.plot(x_grid, y_pred2, color='#004aad', label='Fitted regression line 2')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Flipper length (mm)')\n",
    "plt.ylabel('Body mass (g)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7c135",
   "metadata": {},
   "source": [
    "By comparing these fitted lines, we can directly see how the regression model varies from sample to sample due to **random sampling variability**, even though the assumed relationship is the same.\n",
    "\n",
    "Now let’s repeat this procedure **1000 times** and overlay all fitted regression lines on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d9b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation to illustrate why confidence bands are curved\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of simulations and sample size\n",
    "n_simulations = 1000\n",
    "sample_size = 50\n",
    "\n",
    "# Create a grid of x values for plotting the fitted lines\n",
    "x_grid = np.linspace(penguins['flipper_length_mm'].min(), penguins['flipper_length_mm'].max(), 100)\n",
    "\n",
    "# Plot the original data (light gray for context)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], color='lightgray', alpha=0.3)\n",
    "\n",
    "# Run the simulation\n",
    "for simulation in range(n_simulations):\n",
    "    # Randomly sample 50 penguins\n",
    "    sample = penguins.sample(n=sample_size, replace=True)\n",
    "    \n",
    "    # Fit the same simple linear regression model\n",
    "    model_sim = smf.ols('body_mass_g ~ flipper_length_mm', data=sample).fit()\n",
    "    \n",
    "    # Predict body mass over the x grid\n",
    "    y_pred = model_sim.predict({'flipper_length_mm': x_grid})\n",
    "    \n",
    "    # Plot the fitted line (very transparent)\n",
    "    plt.plot(x_grid, y_pred, color='#004aad')\n",
    "\n",
    "# Plot labels\n",
    "plt.xlabel('Flipper length (mm)')\n",
    "plt.ylabel('Body mass (g)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e49be5",
   "metadata": {},
   "source": [
    "As you can see, even though every fitted line is straight, together they fill out a **curved envelope**.\n",
    "\n",
    "This shape arises because:\n",
    "- Near $\\bar{x}$, the fitted lines are tightly clustered, indicating **small uncertainty**.  \n",
    "- Far from $\\bar{x}$, the fitted lines diverge in both directions, indicating **larger uncertainty**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45d2bd",
   "metadata": {},
   "source": [
    "Under the usual linear regression assumptions, we can say that we are **95% confident** that the true best-fit line lies within these curved boundaries.  \n",
    "However, remember that these **confidence bands refer to the mean response**, not individual data points. Many observations will fall outside the band, as you can also see here.  \n",
    "If we wanted to capture most of the *data points* instead, we would use a **prediction interval**, which is considerably wider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76eb01f",
   "metadata": {},
   "source": [
    "##### Visualizing the prediction bands\n",
    "\n",
    "**Prediction bands** show how the **uncertainty for an individual outcome** $Y$ changes across values of $x$.  \n",
    "\n",
    "Unlike **confidence bands**, which describe uncertainty in the **mean response** $\\hat{y}(x)$, prediction bands answer a different question:\n",
    "\n",
    "> *“If $X = x$, where might the outcome of a **new individual** fall?”*\n",
    "\n",
    "**Interpretation at a fixed $x$:**  \n",
    "A **95% prediction interval** at $X = x$ means that, for individuals in the population with this same value of $x$, about **95% of their outcomes** are expected to fall **within the interval**.\n",
    "\n",
    "Another way to think about it is this:  \n",
    "If we repeatedly observe **new individuals** with the same $x$, their values of $Y$ would lie inside the prediction interval **about 95% of the time**.\n",
    "\n",
    "Prediction bands are **wider than confidence bands** because they account for **two sources of uncertainty**:\n",
    "1. Uncertainty in the **estimated mean response** $\\hat{y}(x)$.  \n",
    "2. The **natural variability between individuals**, represented by the error variance $\\sigma^2$.\n",
    "\n",
    "We can compute and plot **prediction bands** in Python using the same `.get_prediction()` and `.summary_frame()` output, but this time using the columns `obs_ci_lower` and `obs_ci_upper`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb807f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model predictions and prediction intervals for each observation\n",
    "pred_ols = model1.get_prediction()\n",
    "sf = pred_ols.summary_frame(alpha=0.05)\n",
    "\n",
    "# Sorting by x to ensure the lines and bands are plotted correctly\n",
    "order = np.argsort(penguins['flipper_length_mm'].values)\n",
    "x_sorted = penguins['flipper_length_mm'].values[order]\n",
    "y_sorted = penguins['body_mass_g'].values[order]\n",
    "mean_sorted = sf['mean'].values[order]\n",
    "pi_low_sorted = sf['obs_ci_lower'].values[order]\n",
    "pi_high_sorted = sf['obs_ci_upper'].values[order]\n",
    "\n",
    "# Plotting the prediction band\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], s=15, alpha=0.6, label='observed data', color='darkgray')\n",
    "ax.plot(x_sorted, mean_sorted, lw=2, color='#570a6d', label='fitted line (mean)')\n",
    "ax.fill_between(x_sorted, pi_low_sorted, pi_high_sorted, alpha=0.2, color='#570a6d', label='95% prediction band')\n",
    "\n",
    "ax.set_xlabel('Flipper length (mm)')\n",
    "ax.set_ylabel('Body mass (g)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb88060",
   "metadata": {},
   "source": [
    "> We can also visualize both bands on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting model predictions, confidence intervals, and prediction intervals\n",
    "pred_ols = model1.get_prediction()\n",
    "sf = pred_ols.summary_frame(alpha=0.05)\n",
    "\n",
    "# Sort by flipper_length_mm so lines and bands render correctly\n",
    "order = np.argsort(penguins['flipper_length_mm'].values)\n",
    "x_sorted = penguins['flipper_length_mm'].values[order]\n",
    "y_sorted = penguins['body_mass_g'].values[order]\n",
    "mean_sorted = sf['mean'].values[order]\n",
    "ci_low_sorted = sf['mean_ci_lower'].values[order]\n",
    "ci_high_sorted = sf['mean_ci_upper'].values[order]\n",
    "pi_low_sorted = sf['obs_ci_lower'].values[order]\n",
    "pi_high_sorted = sf['obs_ci_upper'].values[order]\n",
    "\n",
    "# Plot confidence and prediction bands together\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(penguins['flipper_length_mm'], penguins['body_mass_g'], s=15, alpha=0.6, label='observed data', color='darkgray')\n",
    "ax.plot(x_sorted, mean_sorted, lw=2, color='#004aad', label='fitted line (mean)')\n",
    "ax.fill_between(x_sorted, ci_low_sorted, ci_high_sorted, alpha=0.25, color='#004aad', label='95% CI (mean)')\n",
    "ax.fill_between(x_sorted, pi_low_sorted, pi_high_sorted, alpha=0.18, color='#570a6d', label='95% prediction band')\n",
    "ax.set_xlabel('Flipper length (mm)')\n",
    "ax.set_ylabel('Body mass (g)')\n",
    "ax.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fab598",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 3: Modeling toxicity using chemical properties</font>\n",
    "\n",
    "In this task, we will put the theory into practice.\n",
    "\n",
    "Our goal is to develop a statistical model to predict **chemical toxicity** for fathead minnows. Toxicity is measured as **$\\text{LC}_{50}$**, which represents the concentration of a chemical that causes **50% mortality**.\n",
    "\n",
    "Based on scientific knowledge from environmental chemistry and ecotoxicology, we know that **chemical hydrophobicity** plays a key role in toxicity. A common measure of hydrophobicity is the **octanol/water partition coefficient** ($K_\\text{OW}$), which is often used in its logarithmic form, **$\\log P$**. More hydrophobic chemicals tend to bioaccumulate more easily and may therefore be more toxic to aquatic organisms.\n",
    "\n",
    "To do this, you are provided with a file `toxicity.tsv`, which contains measurements for multiple chemicals:\n",
    "- octanol/water partition coefficient ($K_\\text{OW}$)\n",
    "- $\\text{LC}_{50}$ values for fathead minnows\n",
    "\n",
    "In the following steps, we will use this dataset to explore the relationship between chemical hydrophobicity and toxicity, check the underlying statistical assumptions, and build an appropriate regression model.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Exploring the relationship between variables**\n",
    "\n",
    "First, we investigate whether there is a **linear relationship** between toxicity ($\\text{LC}_{50}$) and hydrophobicity ($K_\\text{OW}$), as suggested by theory.\n",
    "\n",
    "To do this, we will:\n",
    "- **Visually inspect** the relationship using an appropriate plot.\n",
    "- Compute the **Pearson correlation coefficient** to quantify the strength and direction of the linear association.\n",
    "\n",
    "> Keep in mind that the Pearson correlation is a **parametric measure**, which relies on certain assumptions.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44176afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the toxicity dataset\n",
    "toxicity = pd.read_csv('toxicity.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a35771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to examine the relationship between the variables,\n",
    "# and histograms to check whether they are approximately normally distributed,\n",
    "# which is an assumption of Pearson’s correlation coefficient\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(data=toxicity, x='Kow', y='LC50 (mmol/L)', s=25, color='#fc7202', ax=axes[0])\n",
    "axes[0].set_xlabel('Kow')\n",
    "axes[0].set_ylabel('LC50 (mmol/L)')\n",
    "axes[0].set_title('Relationship between Kow and LC50')\n",
    "\n",
    "# Histograms\n",
    "sns.histplot(toxicity['Kow'], bins=20, color='darkgray', ax=axes[1])\n",
    "axes[1].set_xlabel('Kow')\n",
    "axes[1].set_title('Distribution of Kow')\n",
    "\n",
    "sns.histplot(toxicity['LC50 (mmol/L)'], bins=20, color='darkgray', ax=axes[2])\n",
    "axes[2].set_xlabel('LC50 (mmol/L)')\n",
    "axes[2].set_title('Distribution of LC50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a65ec",
   "metadata": {},
   "source": [
    "> Based on the plots, we can see that **neither variable is normally distributed**. Both **$K_\\text{OW}$** and **$\\text{LC}_{50}$** show strong right skewness, with most observations concentrated at low values and a few very large values.\\\n",
    "> This pattern is consistent with what we would expect for **concentration-related variables**, which often follow a **log-normal distribution** rather than a normal one.\\\n",
    "> Because the assumption of normality is violated, using the raw variables is not ideal for Pearson correlation or linear regression. A common and scientifically meaningful solution is to apply a **log-transformation** to both variables.\\\n",
    "> Therefore, in the next step, we will **log-transform $K_\\text{OW}$ and $\\text{LC}_{50}$** and then repeat the visualizations to check whether the transformed variables are closer to being normally distributed and whether the relationship between\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the variables to achieve approximate normality\n",
    "toxicity['logP'] = np.log10(toxicity['Kow'])\n",
    "toxicity['log_LC50'] = np.log10(toxicity['LC50 (mmol/L)'])\n",
    "\n",
    "toxicity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaa6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "# Scatter plot\n",
    "sns.scatterplot(data=toxicity, x='logP', y='log_LC50', s=25, color='#fc7202', ax=axes[0])\n",
    "axes[0].set_xlabel('logP')\n",
    "axes[0].set_ylabel('log(LC50)')\n",
    "axes[0].set_title('Relationship between logP and log(LC50)')\n",
    "\n",
    "# Histograms\n",
    "sns.histplot(toxicity['logP'], bins=20, color='darkgray', ax=axes[1])\n",
    "axes[1].set_xlabel('logP')\n",
    "axes[1].set_title('Distribution of logP')\n",
    "\n",
    "sns.histplot(toxicity['log_LC50'], bins=20, color='darkgray', ax=axes[2])\n",
    "axes[2].set_xlabel('log(LC50)')\n",
    "axes[2].set_title('Distribution of log(LC50)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4832fcb",
   "metadata": {},
   "source": [
    "> After applying the logarithmic transformation, we can see a clear improvement in the distributions of both variables. The heavy right skew observed in the original scale is no longer present, and the data are much more symmetrically distributed.\\\n",
    "> The scatterplot of **$\\log P$** versus **$\\log (\\text{LC}_{50})$** also suggests a more linear relationship, which is more consistent with the assumptions of Pearson correlation and linear regression.\\\n",
    "> However, from histograms alone it is still **visually difficult to determine** whether the transformed variables are approximately normally distributed. To assess normality more formally, we will therefore also examine **Q–Q plots**, which allow us to compare the empirical distributions of the transformed variables to a theoretical normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Q-Q plot for logP\n",
    "res1 = stats.probplot(toxicity['logP'], dist='norm', plot=axes[0])\n",
    "axes[0].get_lines()[0].set_markerfacecolor('#fc7202')\n",
    "axes[0].get_lines()[0].set_markeredgecolor('#fc7202')\n",
    "axes[0].get_lines()[1].set_color('black')\n",
    "axes[0].get_lines()[1].set_linestyle('--')\n",
    "axes[0].set_title('Q-Q Plot: logP', fontsize=11)\n",
    "axes[0].set_xlabel('Theoretical quantiles')\n",
    "axes[0].set_ylabel('Sample quantiles')\n",
    "\n",
    "# Q-Q plot for log(LC50)\n",
    "res2 = stats.probplot(toxicity['log_LC50'], dist='norm', plot=axes[1])\n",
    "axes[1].get_lines()[0].set_markerfacecolor('#fc7202')\n",
    "axes[1].get_lines()[0].set_markeredgecolor('#fc7202')\n",
    "axes[1].get_lines()[1].set_color('black')\n",
    "axes[1].get_lines()[1].set_linestyle('--')\n",
    "axes[1].set_title('Q-Q Plot: log(LC50)', fontsize=11)\n",
    "axes[1].set_xlabel('Theoretical quantiles')\n",
    "axes[1].set_ylabel('Sample quantiles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676ae27",
   "metadata": {},
   "source": [
    "> Based on the Q–Q plots, the **log-transformed variables align closely with the 45° reference line**, indicating that the data are **approximately normally distributed on the log scale**.\\\n",
    "> Of course, to formally confirm normality we would need to apply dedicated statistical tests, such as the **Shapiro–Wilk** or **Kolmogorov–Smirnov** tests. However, for the purposes of this analysis, we proceed under the reasonable assumption that the variables are now **approximately normally distributed**.\n",
    ">\n",
    "> This allows us to **compute the Pearson correlation coefficient** and to test whether the relationship between **$\\log P$** and **$\\log (\\text{LC}_{50})$** is **statistically significant**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1165f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_r, pearson_p = pearsonr(toxicity['logP'], toxicity['log_LC50'])\n",
    "\n",
    "print(f\"Correlation (logP vs log(LC50)): r = {pearson_r:.3f}, p = {pearson_p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19f730",
   "metadata": {},
   "source": [
    "> Based on these results, we observe a **strong negative linear relationship** between **$\\log P$** and **$\\log (\\text{LC}_{50})$**, with a Pearson correlation coefficient of $r = -0.818$.\n",
    "> \n",
    "> The associated *p*-value ($p < 0.05$) indicates that this relationship is **statistically significant**, meaning it is very unlikely to have arisen by chance.\\\n",
    "> In practical terms, this suggests that **more hydrophobic chemicals (higher $\\log P$)** tend to be **more toxic** to fathead minnows (lower $\\log (\\text{LC}_{50})$ values).\n",
    ">\n",
    "> Given the strength and statistical significance of this relationship, it is reasonable to proceed to the next step and **build a linear regression model** to predict toxicity based on chemical hydrophobicity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650fa77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Step 2: Building the statistical model**\n",
    "\n",
    "Now that we have identified a strong and statistically significant linear relationship between **$\\log P$** and **$\\log (\\text{LC}_{50})$**, we can proceed with building a **statistical model**.\n",
    "\n",
    "In this step, we will:\n",
    "- **Fit a linear regression model** with **$\\log (\\text{LC}_{50})$** as the dependent (response) variable and **$\\log P$** as the predictor (independent variable).\n",
    "- **Examine the model summary** to interpret the estimated coefficients, overall model fit, and statistical significance.\n",
    "- **Analyze the residuals** to check key regression assumptions, including linearity, constant variance, and approximate normality of errors.\n",
    "- **Visualize the fitted model**, including both the **confidence band** for the mean response and the **prediction band** for individual observations, on the same figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fee659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS on the log scale\n",
    "model_toxicity = smf.ols('log_LC50 ~ logP', data=toxicity).fit()\n",
    "print(model_toxicity.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0a1e7",
   "metadata": {},
   "source": [
    "> The fitted **OLS regression model** explains a substantial portion of the variability in **$\\log (\\text{LC}_{50})$**. The coefficient of determination is $R^2 = 0.670$, meaning that approximately **67% of the variance in toxicity** is explained by **$\\log P$** alone. \n",
    ">\n",
    "> The overall model is **statistically significant**, as shown by the *F*-test ($F = 85.22$, $p < 0.05$). This confirms that **$\\log P$** provides meaningful explanatory power for predicting **$\\log (\\text{LC}_{50})$**.\n",
    ">\n",
    "> Interpretation of coefficients:\n",
    "> - The **coefficient** for **$\\log P$** is **−0.903**, and it is highly statistically significant ($p < 0.05$). This indicates a strong **negative relationship** between hydrophobicity and toxicity. As **$\\log P$** increases by one unit, **$\\log (\\text{LC}_{50})$** decreases on average by about **0.9 units**, implying higher toxicity for more hydrophobic chemicals. (The **95% confidence interval** for the $\\log P$ coefficient ranges from **−1.10 to −0.71**, and it does not include zero. This further supports the conclusion that the effect of **$\\log P$** on toxicity is statistically significant.)\n",
    "> - The **intercept** represents the expected value of **$\\log (\\text{LC}_{50})$** when **$\\log P = 0$**. While this value may not have a direct physical interpretation, it is statistically significant and necessary for defining the regression line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "fitted = model_toxicity.fittedvalues\n",
    "resid = model_toxicity.resid\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Residuals vs fitted\n",
    "axes[0].scatter(fitted, resid, s=18, color='#fc7202')\n",
    "axes[0].axhline(0, color='black', lw=1)\n",
    "axes[0].set_xlabel('Fitted values ŷ')\n",
    "axes[0].set_ylabel('Residuals e')\n",
    "axes[0].set_title('Residuals vs Fitted (log model)')\n",
    "\n",
    "# Q–Q plot of residuals using probplot\n",
    "stats.probplot(resid, dist=\"norm\", plot=axes[1])\n",
    "axes[1].get_lines()[0].set_markerfacecolor('#fc7202')\n",
    "axes[1].get_lines()[0].set_markeredgecolor('#fc7202')\n",
    "axes[1].get_lines()[1].set_color('black')\n",
    "axes[1].get_lines()[1].set_linestyle('--')\n",
    "axes[1].set_title('Q–Q Plot of Residuals (log model)')\n",
    "axes[1].set_xlabel('Theoretical quantiles')\n",
    "axes[1].set_ylabel('Sample quantiles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182c161",
   "metadata": {},
   "source": [
    "> As discussed earlier, for a linear regression model to be appropriate, the **residuals should behave like random noise** and be **approximately normally distributed**.\\\n",
    "> In the **residuals vs fitted** plot, the residuals are scattered almost randomly around zero with no clear systematic pattern. This supports the assumptions of **linearity** and **constant variance** of the errors.\\\n",
    "> In the **Q–Q plot**, the residuals lie close to the reference line, with only small deviations in the tails. This suggests that the assumption of **approximate normality of the residuals** is reasonably satisfied.\n",
    ">\n",
    ">Taken together, these diagnostics indicate that the model assumptions are met to a sufficient degree, and the fitted linear regression model can be considered **appropriate for inference and prediction** in this context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the toxicity model with its confidence and prediction bands\n",
    "pred = model_toxicity.get_prediction()\n",
    "sf = pred.summary_frame(alpha=0.05)\n",
    "\n",
    "# Sorting by x so that the line and bands plot correctly\n",
    "order = np.argsort(toxicity['logP'].values)\n",
    "x_sorted = toxicity['logP'].values[order]\n",
    "y_sorted = toxicity['log_LC50'].values[order]\n",
    "mean_sorted = sf['mean'].values[order]\n",
    "ci_low_sorted = sf['mean_ci_lower'].values[order]\n",
    "ci_high_sorted = sf['mean_ci_upper'].values[order]\n",
    "pi_low_sorted = sf['obs_ci_lower'].values[order]\n",
    "pi_high_sorted = sf['obs_ci_upper'].values[order]\n",
    "\n",
    "# Plotting the data, fitted line, confidence band, and prediction band\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plotting observed data points\n",
    "ax.scatter(toxicity['logP'], toxicity['log_LC50'], s=15, alpha=0.6, color='darkgray', label='Observed data')\n",
    "\n",
    "# Plotting fitted regression line\n",
    "ax.plot(x_sorted, mean_sorted, lw=2, color='#004aad', label='Fitted line (mean)')\n",
    "\n",
    "# Adding 95% confidence band for the mean response\n",
    "ax.fill_between(x_sorted, ci_low_sorted, ci_high_sorted, alpha=0.25, color='#004aad', label='95% CI (mean)')\n",
    "\n",
    "# Adding 95% prediction band for individual observations\n",
    "ax.fill_between(x_sorted, pi_low_sorted, pi_high_sorted, alpha=0.15, color='#570a6d', label='95% prediction band')\n",
    "\n",
    "ax.set_xlabel('logP')\n",
    "ax.set_ylabel('log(LC50)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9e822",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 4: Predicting toxicity for new chemicals</font>\n",
    "\n",
    "Now that we have built and evaluated a statistical model, the natural next step is to **use it for prediction**.\n",
    "\n",
    "You are provided with data for **10 new chemicals** in the file `sample10.tsv`. For these chemicals, the value of **$\\log P$** is known, but their toxicity has not yet been measured.\n",
    "\n",
    "We will use the fitted model to predict their **$\\text{LC}_{50}$** values.\n",
    "\n",
    "*Reminder:* The model was developed on the **log scale** and predicts **$\\log (\\text{LC}_{50})$**. Therefore, after making the predictions, the results must be **transformed back to the original scale** in order to obtain $\\text{LC}_{50}$ values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new chemical data for prediction\n",
    "new_chemicals = pd.read_csv('sample10.tsv', sep='\\t')\n",
    "new_chemicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d21da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions (log10(LC50)) for new chemicals\n",
    "pred_log = model_toxicity.predict(new_chemicals['logP'])\n",
    "\n",
    "# Adding predictions to the DataFrame\n",
    "new_chemicals['pred_log_LC50'] = pred_log\n",
    "\n",
    "# Back-transforming to the original LC50 scale (mmol/L)\n",
    "new_chemicals['pred_LC50_mmol_L'] = 10 ** new_chemicals['pred_log_LC50']\n",
    "new_chemicals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad66e4",
   "metadata": {},
   "source": [
    "##### How accurate is our model?\n",
    "\n",
    "Now that we have obtained the **predicted $\\text{LC}_{50}$ values** for the 10 new chemicals, the next question is: **how accurate is our model?**  \n",
    "\n",
    "To evaluate its predictive performance, we can compare the model’s predictions with the **experimentally measured $\\text{LC}_{50}$ values** using a quantitative error metric.\n",
    "\n",
    "One of the most widely used measures is the **Root Mean Squared Error (RMSE)**.  \n",
    "RMSE quantifies the **standard deviation of the prediction errors** (residuals), indicating how far the predicted values typically deviate from the observed data points.  \n",
    "Because RMSE is expressed in the **same units** as the original measurements, it is directly interpretable and widely used to assess model accuracy.  \n",
    "\n",
    "Mathematically, for true values $y_i$ and predicted values $\\hat{y}_i$ ($i = 1, \\dots, n$), the RMSE is defined as: $$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} \\big(\\hat{y}_i - y_i\\big)^2 }.$$  \n",
    "\n",
    "To compute RMSE, we need the **true (experimental) $\\text{LC}_{50}$ values**.  \n",
    "For these 10 chemicals, they are provided in the file `sample10_experimental_LC50.tsv`.  \n",
    "\n",
    "Once both the predicted and experimental values are available, the RMSE can be computed in Python using the function `root_mean_squared_error()` from `sklearn.metrics`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experimental LC50 values for the new chemicals\n",
    "experimental_values = pd.read_csv('sample10_experimental_LC50.tsv', sep='\\t')\n",
    "experimental_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging experimental LC50 values with model predictions based on matching chemical name and CAS\n",
    "new_chemicals = new_chemicals.merge(experimental_values, how='inner', on=['Chemical name', 'CAS'])\n",
    "new_chemicals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d666508",
   "metadata": {},
   "source": [
    "Before calculating the **RMSE**, let’s first **visualize** the predicted and experimental $\\text{LC}_{50}$ values for the 10 new chemicals.  \n",
    "\n",
    "We’ll use the same plotting style as before: we will show the **fitted regression line**, the **confidence band** and the **prediction band**, and then overlay both the **predicted** and the **true (experimental)** values for the new chemicals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing model fit, uncertainty bands, and the new predictions vs. experimental data\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Plotting observed data used for model fitting\n",
    "ax.scatter(toxicity['logP'], toxicity['log_LC50'], s=15, alpha=0.6, color='darkgray', label='Training data')\n",
    "\n",
    "# Plotting the fitted regression line (mean response)\n",
    "ax.plot(x_sorted, mean_sorted, lw=2, color='#004aad', label='Fitted line (mean)')\n",
    "\n",
    "# Adding 95% confidence band for the mean response\n",
    "ax.fill_between(x_sorted, ci_low_sorted, ci_high_sorted, alpha=0.25, color='#004aad', label='95% CI (mean)')\n",
    "\n",
    "# Adding 95% prediction band for individual observations\n",
    "ax.fill_between(x_sorted, pi_low_sorted, pi_high_sorted, alpha=0.15, color='#570a6d', label='95% prediction band')\n",
    "\n",
    "# Plotting predicted LC50 values (log scale) for new chemicals\n",
    "ax.scatter(new_chemicals['logP'], new_chemicals['pred_log_LC50'], s=30, color='#fc7202', label='Predicted values', zorder=8)\n",
    "\n",
    "# Plotting experimental LC50 values (log scale) for new chemicals\n",
    "ax.scatter(new_chemicals['logP'], np.log10(new_chemicals['LC50 (mmol/L)']), s=30, color='#1b7173', label='Experimental values', zorder=9)\n",
    "\n",
    "ax.set_xlabel('logP')\n",
    "ax.set_ylabel('log(LC50)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6635793",
   "metadata": {},
   "source": [
    "> From the plot, we see that the predicted $\\text{LC}_{50}$ values lie exactly on the regression line, as they are obtained directly from the model.  \n",
    "> The experimental $\\text{LC}_{50}$ values, however, deviate to varying degrees: depending on the chemical, some experimental values differ substantially from the model predictions, while others align more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfeae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating RMSE between predicted and experimental LC50 values\n",
    "root_mean_squared_error(new_chemicals['LC50 (mmol/L)'], new_chemicals['pred_LC50_mmol_L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c2cd7",
   "metadata": {},
   "source": [
    "The RMSE is ≈ 0.237 mmol/L, which means that, on average, the model’s LC50 predictions differ from the experimental values by about 0.24 mmol/L."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42433f7",
   "metadata": {},
   "source": [
    "##### How do we evaluate a model when no additional data are available?\n",
    "\n",
    "In many real-world applications, we face an important practical problem: we have **only one dataset**, and after building the model we **cannot collect new data** to independently evaluate its performance.\n",
    "\n",
    "This is actually the **most common scenario in practice**. We are given a dataset, we need to:\n",
    "1. build a model,\n",
    "2. assess how well it performs, and\n",
    "3. then apply it to make predictions for new, unseen cases.\n",
    "\n",
    "To do this correctly, we must plan for model evaluation **from the very beginning**.\n",
    "\n",
    "--- \n",
    "\n",
    "**Holding out data for validation**\n",
    "\n",
    "When no external validation data are available, the standard solution is to **set aside part of the original dataset** before fitting the model.\n",
    "\n",
    "- One part of the data is used for **model fitting** (also called *training* or *calibration*).\n",
    "- The remaining part is kept **completely separate** and is used **only at the end** to evaluate the model’s predictive performance (*testing*/*validation* data).\n",
    "\n",
    "Importantly, the test data **must not be used** during model fitting and should be examined **only after the model is finalized**. (This mimics the situation of predicting truly new data.)\n",
    "\n",
    "In addition, the **held-out test set must be representative** of the original dataset.  \n",
    "This means it should cover a similar range of predictor values and reflect the same variability as the full data. If the test set contains only extreme or atypical observations, the resulting performance assessment can be misleading.\n",
    "\n",
    "For this reason, data splitting is typically done **randomly**. In some cases, **stratified splitting** is used, where the data are split in a way that preserves the distribution of key variables (for example, ranges of toxicity or predictor values) in both the training and test sets.\n",
    "\n",
    "--- \n",
    "\n",
    "**How much data should be set aside?**\n",
    "\n",
    "The exact strategy depends on **how much data we have available**:\n",
    "\n",
    "- **Large datasets**  \n",
    "  A simple **train–test split** is often sufficient, for example:\n",
    "  - 70% training / 30% testing  \n",
    "  - 80% training / 20% testing  \n",
    "\n",
    "- **Moderate-sized datasets**  \n",
    "  A train–test split is still possible, but the results may depend strongly on *which* observations end up in the test set.\n",
    "\n",
    "- **Small datasets**  \n",
    "  Setting aside a large test set can waste valuable information and leave too little data for model fitting.  \n",
    "  On the other hand, setting aside a **very small validation set** can also be problematic, because performance estimates become **unstable and highly sensitive** to which observations happen to be included.  \n",
    "\n",
    "  In such cases, more efficient approaches, such as **cross-validation**, are often preferred because they make better use of the limited data while still providing a meaningful assessment of model performance.\n",
    "\n",
    "--- \n",
    "\n",
    "**Common validation strategies**\n",
    "\n",
    "Depending on data size and context, common options include:\n",
    "\n",
    "- **Train–test split**  \n",
    "  Simple and intuitive, but sensitive to how the split is done.\n",
    "\n",
    "- **Cross-validation** (for example, *k*-fold cross-validation)  \n",
    "  The data are split into several parts, and the model is trained and tested multiple times.  \n",
    "  This uses data more efficiently and provides a more stable estimate of model performance.\n",
    "  <p align=\"left\">\n",
    "  <img src=\"https://raw.githubusercontent.com/RaHub4AI/MI7032/refs/heads/main/Pictures/k_fold_cv.png\"\n",
    "       alt=\"cv\"\n",
    "       width=\"800\">\n",
    "</p>\n",
    " \n",
    "- **Leave-one-out cross-validation (LOOCV)**  \n",
    "  A special case of cross-validation, often used when datasets are very small.  \n",
    "  In LOOCV, the model is trained on all observations **except one**, and the left-out observation is used for testing.  \n",
    "  This process is repeated so that **each observation is used once as the test case**, and the prediction errors are then aggregated.\n",
    "\n",
    "Each approach represents a trade-off between **bias**, **variance**, and **data efficiency**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75797c6c",
   "metadata": {},
   "source": [
    "#### <font color=\"#fc7202\">Task 5: Model evaluation</font>\n",
    "In this example, we will look at one of the **most common ways to split data in Python**.\n",
    "\n",
    "We will use the function `train_test_split()` from `sklearn.model_selection` to divide the original toxicity dataset, which contains **44 chemicals**, into:\n",
    "- a **calibration (training) set** and\n",
    "- a **validation (test) set**.\n",
    "\n",
    "We will apply a **70:30 split**, meaning that 70% of the data are used to fit the model and the remaining 30% are held out for validation. Although this dataset is relatively small, this example clearly illustrates the standard workflow.\n",
    "\n",
    "After splitting the data, we will:\n",
    "- fit the regression model using only the calibration data,\n",
    "- generate predictions for the validation set, and\n",
    "- evaluate model performance by reporting the **RMSE** on the validation data.\n",
    "\n",
    "This procedure mimics how models are typically developed and evaluated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebce4cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into calibration (70%) and validation (30%) sets\n",
    "train_df, test_df = train_test_split(toxicity, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b833240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression model on the calibration data\n",
    "model_small = smf.ols('log_LC50 ~ logP', data=train_df).fit()\n",
    "\n",
    "print(model_small.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd341c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict log(LC50) for the validation data\n",
    "log_lc50_pred = model_small.predict(test_df)\n",
    "\n",
    "# Transform predictions and observations back to LC50 scale\n",
    "lc50_pred = np.exp(log_lc50_pred)\n",
    "lc50_true = np.exp(test_df['log_LC50'])\n",
    "\n",
    "# Compute RMSE on the validation set\n",
    "rmse = root_mean_squared_error(lc50_true, lc50_pred)\n",
    "\n",
    "print(f'Validation RMSE: {rmse:.3f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "modeling_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
